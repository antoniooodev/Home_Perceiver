# Home PerceiverÂ â€“ Realâ€‘Time Privacyâ€‘Aware Perception Module

> **TL;DR**Â A crossâ€‘platform stack (PyTorchÂ â†’ OpenCV) that runs
> in real time onÂ GPU/CPU/MPS, detects objects & people, segments persons for
> privacy, tracks IDs, estimates poses, exports structured logs, and builds
> PDF reports â€“ all via three demo scripts and a modular PythonÂ API.

---

\##Â Features

| Capability                            | Mode      | Model             | Notes                     |
| ------------------------------------- | --------- | ----------------- | ------------------------- |
| ğŸŸ¥ Object detection (80Â COCO classes) | **Multi** | YOLOv5â€‘nano       | 35â€¯FPS @Â 640Â px           |
| ğŸŸ¦ Person instance segmentation       | **A / B** | YOLOv8â€‘nanoâ€‘seg   | tight privacy mask        |
| ğŸŸ¨ Pose estimation (17â€‘pt)            | **A**     | KeypointÂ Râ€‘CNN    | optional skeleton overlay |
| ğŸŸ© Multiâ€‘person tracking              | **A**     | IoU tracker       | persistent IDs            |
| ğŸ“ˆ Structured export                  | all       | JSONLÂ + CSVÂ + MP4 | one line per frame        |
| ğŸ“„ Report builder                     | tools     | LaTeX             | summary PDF with plots    |

Tested on **macOSÂ 14Â (M2Â Pro)**, **WindowsÂ 11 (RTXÂ 3060Â Ti)** and **UbuntuÂ 22.04
(CUDAÂ 12)**.

---

\##Â Directory layout

```text
visual_detection/
â”‚  main.py                â† entryâ€‘point selector (optional)
â”‚  pyproject.toml         â† editable install
â”‚
â”œâ”€ core_utils/            â† VideoStream + Exporter (logging)
â”œâ”€ detector/              â† YOLO helpers + demo detect.py (legacy)
â”œâ”€ pose/                  â† pose utils + demo script
â”œâ”€ tracker/               â† lightweight IoU tracker
â”œâ”€ scripts/               â† highâ€‘level demos
â”‚Â Â  â”œâ”€ demo_modeA.py      â† maskÂ +Â poseÂ +Â tracking (privacy)
â”‚Â Â  â”œâ”€ demo_modeB.py      â† mask only, multiâ€‘class detection
â”‚Â Â  â””â”€ detect_multiclass.py â† bbox only, fastest
â”œâ”€ tools/                 â† analyze_run.py, build_report.py, â€¦
â”œâ”€ tests/                 â† pytest smoke suite
â””â”€ weights/               â† preâ€‘downloaded *.pt (gitÂ LFS)
```

---

\##Â Installation

\###Â 1Â Â· Clone & create venvÂ (ğŸÂ â‰¥â€¯3.11)

```bash
git clone https://github.com/antoniooodev/Home_Perceiver.git
cd Home_Perceiver
python3.11 -m venv .venv
source .venv/bin/activate
```

\###Â 2Â Â· Install project & deps

```bash
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt      # full stack (~1â€¯GB incl. Torch)
# or: pip install -e .               # editable install (uses pyproject)
```

*AppleÂ SiliconÂ âœ“*Â â€“ the PyTorch wheels include the **MPS** backend; no extra
steps required.

\###Â 3Â Â· Retrieve model weights

```bash
# already under weights/, but you can refresh:
python - <<'PY'
from torchvision.models import get_weight
from ultralytics import YOLO
YOLO('yolov8n-seg.pt')        # autoâ€‘download to ~/.cache/torch/hub
PY
```

_(Large files in `weights/` are tracked via **gitÂ LFS** â€“ run
`brew install gitâ€‘lfs && git lfs install` if needed.)_

\###Â 4Â Â· Verify

```bash
pytest -q          # 3Â âœ“ fast smoke tests
```

---

\##Â Usage

\###Â ModeÂ AÂ â€“ PrivacyÂ maskÂ + poseÂ + tracking

```bash
python scripts/demo_modeA.py \
       --source 0            # 0=webcam / path / rtsp:// â€¦
       --device mps          # cpu | cuda | mps
       --save-video          # optional MP4 in output/videos/
```

\###Â ModeÂ BÂ â€“ Multiâ€‘class detectionÂ + mask persona

```bash
python scripts/demo_modeB.py --source sample.mp4 --no-display
```

\###Â Multiâ€‘class only (fastest)

```bash
python scripts/detect_multiclass.py \
       --source https://â€¦/stream.mjpg --save-video
```

All demos create a **runâ€‘ID** timestamp (e.g. `20250726â€‘1631xxâ€‘abcdef`) and
write:

```
output/json/<run>.jsonl    â† one line per frame
output/csv/<run>.summary.csv
output/videos/<run>.mp4    â† if --save-video
```

---

\##Â Postâ€‘processing

\###Â 1Â Â·Â Aggregate a single run

```bash
python tools/analyze_run.py --run output/json/<run>.jsonl
# â†’ <run>.summary.csv + <run>.classes.png
```

\###Â 2Â Â·Â Generate PDF report

```bash
python tools/build_report.py \
       --run-id <run> \
       --title "Modeâ€‘A â€“ Webcam (MacÂ M2)"
# â†’ output/reports/<run>.pdf (needs pdflatex in PATH)
```

\###Â 3Â Â·Â Batch aggregate

```bash
python tools/aggregate_runs.py --glob "output/json/*.jsonl"
# merges into output/csv/_aggregate.csv
```

---

\##Â Training on custom data Â (optional)

Fineâ€‘tune **YOLOv8â€‘nano** on your dataset (COCOâ€‘style and Homeobj3k) :

```bash
# data.yaml lists train/val and class names
ultralytics yolo detect train \
    model=yolov8n.pt data=data.yaml imgsz=640 epochs=50 batch=16 device=mps
cp runs/detect/train/weights/best.pt weights/yolov8n-custom.pt
```

Update `detector/yolo_utils.py` to load the new weight.

---

\##Â Testing & CI

```bash
pytest -q                  # 3 fast tests (<1Â s)
ruff check .                # lint (PEPâ€‘8 + import order)
```

Set up GitHubÂ Actions with matrix `{macosâ€‘14, ubuntuâ€‘latest}` to run the
smoke suite on push.

---

\##Â Credits & License

- Code Â©Â 2025Â VisualÂ DetectionÂ Team â€“ MIT License (see LICENSE).<br>
- YOLOv5/YOLOv8 Â©Â Ultralytics â€“ GPLv3 weights redistributed via LFS.<br>
- Keypoint Râ€‘CNN weights Â©Â TorchVision (BSDâ€‘3).<br>
- COCO, Objectsâ€‘365 datasets under original licenses.

> For academic use please cite â€œVisual Detection Project â€“ Realâ€‘Time
> Privacyâ€‘Aware Vision Module, 2025â€.
